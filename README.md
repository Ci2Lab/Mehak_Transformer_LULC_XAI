# **Transformer-based Land Use and Land Cover Classification with Explainability using Satellite Imagery**

This repository contains the code for our paper:  
ğŸ“„ **[Transformer-based Land Use and Land Cover Classification with Explainability using Satellite Imagery](https://www.nature.com/articles/s41598-024-67186-4)**  
âœ **Authors:** Mehak Khan, Abdul Hanan, Meruyert Kenzhebay, Michele Gazzea & Reza Arghandeh  
ğŸ“š **Journal:** *Scientific Reports (Nature)*  

In this work, we introduce a **framework** that enhances the efficiency of **Vision Transformer (ViT) and Swin Transformer** models through **transfer learning** and **fine-tuning** techniques.  

Our approach also emphasizes **model interpretability**, ensuring that deep learning decisions in **Land Use and Land Cover (LULC) classification** are both **transparent and understandable**. This is particularly crucial for **forestry, agriculture, and environmental monitoring applications** using satellite imagery.

<p align="center">
  <img src="images/framework_new.png" width="700"/>
</p>

---

## **ğŸ“Œ Key Features**
âœ” **Transformer-based Deep Learning:** Fine-tuned **Vision Transformer (ViT)** and **Swin Transformer** models for satellite image classification.  
âœ” **Explainability with Integrated Gradients:** We leverage **Captumâ€™s Integrated Gradients** to provide interpretability in LULC classification.  
âœ” **Efficient Training Pipeline:** Utilizes **transfer learning** and **fine-tuning** for improved performance.  
âœ” **Application Areas:** Forestry, agricultural monitoring, environmental analysis, and urban planning.  

---

## **ğŸ“‚ Dataset**
We use the **[EuroSAT-RGB](https://github.com/phelber/EuroSAT)** dataset, which contains **RGB satellite images** across **ten different land use classes**. For further validation of our frameworkâ€™s generalization and scalability, we conducted additional experiments using **[PatternNet](https://www-sciencedirect-com.galanga.hvl.no/science/article/pii/S0924271618300042)** dataset.

<p align="center">
  <img src="images/Eurosat_table.png" width="600"/>
</p>

Example images from EuroSAT:
<p align="center">
  <img src="images/Eurosat.png" width="550"/>
</p>

---

## **ğŸ§  Models**
Our framework leverages two transformer-based models:

- **Vision Transformer (ViT)**
- **Swin Transformer**

<p align="center">
  <img src="images/ViT.png" width="600"/>
  <img src="images/SwinT.png" width="600"/>
</p>


---

## **ğŸ” Explainability**
To ensure **model interpretability**, we integrate **Integrated Gradients** from the [Captum Library](https://captum.ai/). This allows us to **visualize feature importance** in the classification process.

---

## **ğŸ“Œ Acknowledgements**
- The **[EuroSAT](https://github.com/phelber/EuroSAT)** and **[PatternNet](https://www-sciencedirect-com.galanga.hvl.no/science/article/pii/S0924271618300042)** datasets are publicly available.
- We use **Vision Transformers (ViT)** and **Swin Transformers**, based on the **[Timm library](https://huggingface.co/timm)** library.
- Explainability is powered by the **[Captum Library](https://captum.ai/)**.

---

## ğŸš€ Repository Structure

```text

â”œâ”€â”€ src/                  # Source modules (dataset, model, training, XAI, etc.)
â”‚   â”œâ”€â”€ eurosat_dataset.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ model_loader.py
â”‚   â”œâ”€â”€ model_utils.py
â”‚   â”œâ”€â”€ training.py
â”‚   â”œâ”€â”€ XAI.py
â”‚   â”œâ”€â”€ reproducibility.py
â”‚   â”œâ”€â”€ plot_helpers.py
â”‚   â””â”€â”€ aggregate.py
â”‚
â”œâ”€â”€ main.py               # One-command training/evaluation entry point
â”œâ”€â”€ ViT_EuroSAT_XAI.ipynb # Step-by-step notebook version
â”œâ”€â”€ requirements.txt      # Core dependencies
â””â”€â”€ README.md             # Project documentation (this file)

---

## **ğŸ“¬ Contact**
For questions or collaborations, feel free to open an issue or reach out!  

ğŸ“§ Email: [mehakkhan3@hotmail.com](mailto:mehakkhan3@hotmail.com)  

---

## **ğŸ“ Citation**
If you find this work useful, please cite our paper:

```bibtex
@article{khan2024transformer,
  title={Transformer-based land use and land cover classification with explainability using satellite imagery},
  author={Khan, Mehak and Hanan, Abdul and Kenzhebay, Meruyert and Gazzea, Michele and Arghandeh, Reza},
  journal={Scientific Reports},
  volume={14},
  number={1},
  pages={16744},
  year={2024},
  publisher={Nature Publishing Group UK London}
}


